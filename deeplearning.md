<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>


**L1 L2 正则化的区别，为什么 L1 产生稀疏矩阵，L2 可以防止过拟合**
    
    ·L0范数是非零元素之和，求解是个NP问题，L1L2是近似解
    ·L1正则化是指权值向量w中各个元素的绝对值之和
    ·L2正则化是指权值向量w中各个元素的平方和然后再求平方根
    过拟合实质上是模型的表示能力“太强”，也就是模型容量太大。
    通过加上正则来降低模型的容量或者表示能力，
    L1约束模型参数分布比较稀疏，L2约束参数分布均匀一点，减少系数个数。
    简单点说，l1限制了模型容量，所以可以减少过拟合;
    l2相当于认为参数服从高斯分布，l1是拉普拉斯分布。
    L1,L2范式来自于对数据的先验知识.如果你认为,你现有的数据来自于高斯分布,
    那么就应该在代价函数中加入数据先验P(x),一般由于推导和计算方便会加入对数似然,
    也就是log(P(x)),然后再去优化,这样最终的结果是,由于你的模型参数考虑了数据先验,
    模型效果当然就更好.哦对了,如果你去看看高斯分布的概率密度函数P(x),
    你会发现取对数后的log(P(x))就剩下一个平方项了,这就是L2范式的由来--高斯先验.
    同样,如果你认为你的数据是稀疏的,不妨就认为它来自某种laplace分布.
    laplace分布是尖尖的分布,服从laplace分布的数据就是稀疏的了
    (只有很小的概率有值,大部分概率值都很小或为0).

**梯度消失和梯度爆炸**

    两种情况下梯度消失经常出现，一是在深层网络中，二是采用了不合适的损失函数，比如sigmoid。梯度爆炸一般出现在深层网络和权值初始化值太大

**LR 的数学原理**
    
    逻辑回归也被称为广义线性回归模型，它与线性回归模型的形式基本上相同，都具有 ax+b，其中a和b是待求参数，其区别在于他们的因变量不同，多重线性回归直接将ax+b作为因变量，即y = ax+b，而logistic回归则通过函数S将ax+b对应到一个隐状态p，p = S(ax+b)，然后根据p与1-p的大小决定因变量的值。这里的函数S就是Sigmoid函数
![Sigmoid](Sigmoid.png)

    逻辑回归一般使用交叉熵作为代价函数。
    $$L = - \frac{1}{n}\sum\limits_{i = 1}^n {[{y_i}\log ({{\hat y}_i}) + (1 - {y_i})\log (1 - {{\hat y}_i})]}$$
    梯度下降法：
    $$\begin{equation}
    \nabla L(w) = \frac{\partial L}{\partial w} = \frac{1}{N}\sum_{i=1}^{N}(h(x_i) - y_i)x_i \\
    w = w - \alpha\cdot \nabla L(w)
    \end{equation}$$

**算法评估指标 ROC曲线，AUC值**

**bagging 和 boosting 区别**

**RF、AdaBoost、GBDT、XGBoost 的区别和联系**

*BN*

    BatchNorm 层对于 input batch 会统计出 mean 和 variance 用于计算 EMA。如果input batch 的 shape 为(B, C, H, W)，统计出的 mean 和 variance 的 shape 为:1 * C * 1 * 1
 

**DenseNet:**
    
    DenseNet提出了一个更激进的密集连接机制：即互相连接所有的层，
    具体来说就是每个层都会接受其前面所有层作为其额外的输入。
    且DenseNet是直接concat来自不同层的特征图，这可以实现特征重用，提升效率，
    这一特点是DenseNet与ResNet最主要的区别。
    
**过拟合**

    判断方法
    过拟合（over-fitting），机器学习模型或者是深度学习模型在训练样本中表现得过于优越，导致在验证数据集以及测试数据集中表现不佳。出现这种现象的主要原因是训练数据中存在噪音或者训练数据太少。
    过拟合问题，根本的原因则是特征维度(或参数)过多，导致拟合的函数完美的经过训练集，但是对新数据的预测结果则较差。

    常见原因
    1）建模样本选取有误，如样本数量太少，选样方法错误，样本标签错误等，导致选取的样本数据不足以代表预定的分类规则；
    2）样本噪音干扰过大，使得机器将部分噪音认为是特征从而扰乱了预设的分类规则；
    3）假设的模型无法合理存在，或者说是假设成立的条件实际并不成立；
    4）参数太多，模型复杂度过高；
    5）对于决策树模型，如果我们对于其生长没有合理的限制，其自由生长有可能使节点只包含单纯的事件数据(event)或非事件数据(no event)，使其虽然可以完美匹配（拟合）训练数据，但是无法适应其他数据集。
    6）对于神经网络模型：
    a)对样本数据可能存在分类决策面不唯一，随着学习的进行,，BP算法使权值可能收敛过于复杂的决策面；
    b)权值学习迭代次数足够多(Overtraining)，拟合了训练数据中的噪声和训练样例中没有代表性的特征。
    
    解决方法
    1）在神经网络模型中，可使用权值衰减的方法，即每次迭代过程中以某个小因子降低每个权值。
    2）提前终止迭代训练：选取合适的停止训练标准，使对机器的训练在合适的程度；
    3）保留验证数据集，对训练成果进行验证；
    4）获取额外数据进行交叉验证；
    5）正则化，即在进行目标函数或代价函数优化时，在目标函数或代价函数。如L1L2正则化，Batch Normalization正则化
    6）Bagging和Boosting  Bagging和Boosting是机器学习中的集成方法，多个模型的组合可以弱化每个模型中的异常点的影响，保留模型之间的通性，弱化单个模型的特性。
    7) 增加噪声 这也是深度学习中的一种避免过拟合的方法（没办法，深度学习模型太复杂，容易过拟合），添加噪声的途径有很多，可以在输入数据上添加，增大数据的多样性，可以在权值上添加噪声，这种方法类似于L2正则化。
    8)权值共享  权值共享最常见的就是在卷积神经网络中，权值共享的目的旨在减小模型中的参数，同时还能较少计算量。在循环神经网络中也用到了权值共享
    
**ResNet**
    
    ResNet50:49个卷积层（3+4+6+3）*3+1=49    
![ResNet](ResNet1.png)

**卷积计算量**
    
    给定一个卷积层 C in * C out * H k * W k  (H k * W k 为 kernel shape), output feature  map shape 为 H * W，
    其 FLOPs 为2 * (C in * H k * W k + 1) * C out * H * W，参数量为 (C in * H k * W k + 1) * C out (1 乘累加 = 2 FLOPs)    
![Conv](卷积核1.png)

**SVM LDA**
![svm](svm1.jpg)
![svm](svm2.jpg)

**其他点**


    
    